{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "343fca2d",
   "metadata": {},
   "source": [
    "# Learning RAG (Retrieval-Augmented Generation) from Scratch\n",
    "\n",
    "## What You'll Learn\n",
    "This notebook teaches you how to build a **Knowledge Base Agent** step by step. By the end, you'll understand:\n",
    "\n",
    "1. **Programming Basics**: How to work with Python data structures (lists, dictionaries)\n",
    "2. **What is RAG**: How to combine retrieval (searching documents) with generation (LLM answers)\n",
    "3. **Embeddings**: How text is converted to numbers for semantic search\n",
    "4. **Building Agents**: How to create a system that can answer questions using external knowledge\n",
    "\n",
    "## The Problem We're Solving\n",
    "Imagine you have a collection of documents (company policies, technical docs, etc.) and you want to ask questions about them. A basic LLM can't answer because it doesn't have access to your documents. **RAG solves this** by:\n",
    "1. Finding relevant documents (retrieval)\n",
    "2. Giving those documents to the LLM as context (augmentation)\n",
    "3. Having the LLM generate an answer based on that context (generation)\n",
    "\n",
    "Let's build this step by step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d83cad",
   "metadata": {},
   "source": [
    "## Step 1: Creating a Small Knowledge Base\n",
    "\n",
    "First, let's create a simple collection of documents. Think of this as a mini-database of information.\n",
    "\n",
    "### What is a Knowledge Base?\n",
    "A **knowledge base** is just a collection of documents/facts that contain information. In real applications, this might be:\n",
    "- Company documentation\n",
    "- Product manuals\n",
    "- FAQs\n",
    "- Research papers\n",
    "\n",
    "### Data Structure Basics\n",
    "We'll use Python **dictionaries** (key-value pairs) stored in a **list**. Each document has:\n",
    "- `id`: A unique identifier\n",
    "- `text`: The actual content\n",
    "- `source`: Where it came from\n",
    "- `date`: When it was written\n",
    "\n",
    "This metadata helps us track where information comes from (provenance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843267ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our knowledge base: a list of dictionaries\n",
    "# Each dictionary represents one document\n",
    "knowledge_base = [\n",
    "    {\n",
    "        'id': 'doc1',\n",
    "        'text': 'LangGraph provides a node-based workflow for composing LLM chains. It supports checkpoints and reducers for reliability.',\n",
    "        'source': 'intro.md',\n",
    "        'date': '2024-10-01'\n",
    "    },\n",
    "    {\n",
    "        'id': 'doc2',\n",
    "        'text': 'Retrieval-Augmented Generation (RAG) combines a retriever and a generator to ground output in real documents.',\n",
    "        'source': 'rag.md',\n",
    "        'date': '2024-11-02'\n",
    "    },\n",
    "    {\n",
    "        'id': 'doc3',\n",
    "        'text': 'Best practices: chunking documents, adding metadata, re-ranking top candidates, and logging retrieval traces.',\n",
    "        'source': 'best_practices.md',\n",
    "        'date': '2025-01-15'\n",
    "    },\n",
    "    {\n",
    "        'id': 'doc4',\n",
    "        'text': 'AI agents use tools to interact with external systems. Tools can be APIs, databases, or custom functions.',\n",
    "        'source': 'agents.md',\n",
    "        'date': '2024-12-10'\n",
    "    },\n",
    "    {\n",
    "        'id': 'doc5',\n",
    "        'text': 'Embeddings convert text into vectors (arrays of numbers) that capture semantic meaning.',\n",
    "        'source': 'embeddings.md',\n",
    "        'date': '2024-11-20'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Let's see what we have\n",
    "print(\"üìö Knowledge Base Contents:\")\n",
    "print(\"=\" * 80)\n",
    "for doc in knowledge_base:\n",
    "    # [:70] means \"show first 70 characters\" to keep output clean\n",
    "    preview = doc['text'][:70] + \"...\" if len(doc['text']) > 70 else doc['text']\n",
    "    print(f\"\\nüîπ ID: {doc['id']}\")\n",
    "    print(f\"   Source: {doc['source']}\")\n",
    "    print(f\"   Date: {doc['date']}\")\n",
    "    print(f\"   Content: {preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4ad126",
   "metadata": {},
   "source": [
    "## Step 2: Understanding Embeddings\n",
    "\n",
    "### The Problem with Keyword Search\n",
    "If someone asks \"How do AI systems use external information?\", traditional keyword search might miss document 4 because it doesn't contain those exact words. But semantically, it's very relevant!\n",
    "\n",
    "### What Are Embeddings?\n",
    "**Embeddings** are a way to convert text into numbers (vectors) that capture meaning. Similar concepts have similar numbers.\n",
    "\n",
    "For example:\n",
    "- \"dog\" and \"puppy\" would have similar embeddings\n",
    "- \"dog\" and \"computer\" would have very different embeddings\n",
    "\n",
    "### Two Approaches We'll Try:\n",
    "\n",
    "1. **Simple Approach (TF-IDF)**: Counts words, fast but less sophisticated\n",
    "2. **Advanced Approach (Sentence Transformers)**: Uses AI to understand meaning\n",
    "\n",
    "Let's implement both so you can see the difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1180278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we need\n",
    "import numpy as np  # For numerical operations\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # For simple text search\n",
    "\n",
    "# We'll try to use advanced embeddings, but have a fallback\n",
    "use_advanced_embeddings = False\n",
    "\n",
    "try:\n",
    "    # This is the advanced approach using AI-powered embeddings\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "    # Load a pre-trained model (downloads automatically first time)\n",
    "    # 'all-MiniLM-L6-v2' is a small, fast model good for learning\n",
    "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    use_advanced_embeddings = True\n",
    "    print(\"‚úÖ Using advanced AI embeddings (Sentence Transformers)\")\n",
    "    print(\"   This understands semantic meaning!\")\n",
    "    \n",
    "except ImportError:\n",
    "    # If sentence-transformers isn't installed, we'll use the simpler approach\n",
    "    print(\"‚ö†Ô∏è  Sentence Transformers not installed\")\n",
    "    print(\"   Using simpler TF-IDF approach (keyword-based)\")\n",
    "    print(\"\\n   To install: pip install sentence-transformers\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c4dbd6",
   "metadata": {},
   "source": [
    "## Step 3: Converting Documents to Embeddings\n",
    "\n",
    "Now we need to convert all our documents into embeddings (numbers) so we can search them.\n",
    "\n",
    "### What's Happening Here:\n",
    "1. **Extract text**: Get just the text content from each document\n",
    "2. **Convert to embeddings**: Turn each text into an array of numbers\n",
    "3. **Normalize**: Scale the numbers so comparisons are fair (like converting inches and feet to meters)\n",
    "\n",
    "### Why Normalize?\n",
    "Without normalization, longer documents would have bigger numbers just because they're longer, not because they're more relevant. Normalizing makes comparisons fair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a852170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract all the text from our documents\n",
    "# This creates a list of just the text strings\n",
    "document_texts = [doc['text'] for doc in knowledge_base]\n",
    "\n",
    "print(f\"üìÑ Processing {len(document_texts)} documents...\")\n",
    "\n",
    "if use_advanced_embeddings:\n",
    "    # ADVANCED METHOD: Use AI to understand meaning\n",
    "    \n",
    "    # Convert each text to a vector (array of numbers)\n",
    "    # convert_to_numpy=True makes it easier to do math operations\n",
    "    embedding_vectors = embedding_model.encode(document_texts, convert_to_numpy=True)\n",
    "    \n",
    "    print(f\"‚úÖ Created embeddings with shape: {embedding_vectors.shape}\")\n",
    "    print(f\"   - {embedding_vectors.shape[0]} documents\")\n",
    "    print(f\"   - {embedding_vectors.shape[1]} dimensions (numbers) per document\")\n",
    "    \n",
    "    # Normalize the vectors (make them unit length)\n",
    "    # This is like adjusting volumes to the same level before comparing\n",
    "    norms = np.sqrt((embedding_vectors**2).sum(axis=1, keepdims=True))\n",
    "    embedding_vectors = embedding_vectors / (norms + 1e-9)  # +1e-9 prevents division by zero\n",
    "    \n",
    "    print(\"‚úÖ Vectors normalized for fair comparison\")\n",
    "    \n",
    "else:\n",
    "    # SIMPLE METHOD: Count words and their importance\n",
    "    \n",
    "    # TF-IDF = Term Frequency-Inverse Document Frequency\n",
    "    # It's fancy counting: common words get lower scores, rare words get higher scores\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words='english',  # Ignore common words like 'the', 'a', 'is'\n",
    "        max_features=100  # Keep only top 100 most important words\n",
    "    )\n",
    "    \n",
    "    # Fit and transform converts text to numbers\n",
    "    tfidf_matrix = vectorizer.fit_transform(document_texts)\n",
    "    \n",
    "    print(f\"‚úÖ Created TF-IDF matrix with shape: {tfidf_matrix.shape}\")\n",
    "    print(f\"   - {tfidf_matrix.shape[0]} documents\")\n",
    "    print(f\"   - {tfidf_matrix.shape[1]} unique important words\")\n",
    "    \n",
    "print(\"\\n‚ú® Knowledge base is ready for searching!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a562add",
   "metadata": {},
   "source": [
    "## Step 4: Building the Retrieval Function\n",
    "\n",
    "This is the heart of RAG! We need a function that:\n",
    "1. Takes a user's question\n",
    "2. Finds the most relevant documents\n",
    "3. Returns them with **provenance** (source information)\n",
    "\n",
    "### What is Provenance?\n",
    "**Provenance** means tracking where information came from. This is crucial because:\n",
    "- Users can verify the information\n",
    "- You can trace incorrect answers back to their source\n",
    "- It builds trust in your AI system\n",
    "\n",
    "### How Similarity Works:\n",
    "We'll use the **dot product** to compare vectors. Think of it like measuring the angle between two arrows:\n",
    "- Similar meaning = small angle = high score\n",
    "- Different meaning = large angle = low score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865b3d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_knowledge_base(user_question, top_k=3):\n",
    "    \"\"\"\n",
    "    Search the knowledge base for documents relevant to the user's question.\n",
    "    \n",
    "    Parameters:\n",
    "    - user_question (str): The question to search for\n",
    "    - top_k (int): How many top results to return (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "    - list: Top matching documents with scores and metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    if use_advanced_embeddings:\n",
    "        # === ADVANCED METHOD ===\n",
    "        \n",
    "        # Step 1: Convert the question to an embedding (same as we did for documents)\n",
    "        question_vector = embedding_model.encode([user_question], convert_to_numpy=True)[0]\n",
    "        \n",
    "        # Step 2: Normalize the question vector\n",
    "        question_norm = np.linalg.norm(question_vector)\n",
    "        question_vector = question_vector / (question_norm + 1e-9)\n",
    "        \n",
    "        # Step 3: Calculate similarity scores\n",
    "        # The @ operator does matrix multiplication (dot product)\n",
    "        # Higher score = more similar = more relevant\n",
    "        similarity_scores = embedding_vectors @ question_vector\n",
    "        \n",
    "    else:\n",
    "        # === SIMPLE METHOD ===\n",
    "        \n",
    "        # Step 1: Convert question using same vectorizer as documents\n",
    "        question_vector = vectorizer.transform([user_question])\n",
    "        \n",
    "        # Step 2: Calculate similarity (how many matching words, weighted by importance)\n",
    "        similarity_scores = (tfidf_matrix @ question_vector.T).toarray().ravel()\n",
    "    \n",
    "    # Step 4: Find top K most similar documents\n",
    "    # enumerate() gives us (index, score) pairs\n",
    "    # sorted() orders by score (highest first because of the minus sign)\n",
    "    # [:top_k] takes only the first K results\n",
    "    ranked_results = sorted(\n",
    "        enumerate(similarity_scores), \n",
    "        key=lambda x: -x[1]  # Sort by score, descending\n",
    "    )[:top_k]\n",
    "    \n",
    "    # Step 5: Format results with provenance\n",
    "    results = []\n",
    "    for doc_index, similarity_score in ranked_results:\n",
    "        # Get the original document\n",
    "        original_doc = knowledge_base[doc_index]\n",
    "        \n",
    "        # Create a result with all info\n",
    "        result = {\n",
    "            'id': original_doc['id'],\n",
    "            'text': original_doc['text'],\n",
    "            'source': original_doc['source'],\n",
    "            'date': original_doc['date'],\n",
    "            'relevance_score': float(similarity_score)  # Convert to regular Python float\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Search function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee0bc04",
   "metadata": {},
   "source": [
    "## Step 5: Testing Our Retrieval System\n",
    "\n",
    "Let's try searching! We'll ask a question and see which documents are retrieved.\n",
    "\n",
    "### What to Look For:\n",
    "1. **Relevance Score**: Higher = more relevant (typical range: 0.0 to 1.0)\n",
    "2. **Source**: Which document did this come from?\n",
    "3. **Date**: How recent is this information?\n",
    "\n",
    "This is **RETRIEVAL** - the \"R\" in RAG. We're finding relevant documents, not generating answers yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9765eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test with a few different questions\n",
    "test_questions = [\n",
    "    \"How does RAG work?\",\n",
    "    \"What are tools in AI agents?\",\n",
    "    \"Tell me about embeddings\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"üîç Question: {question}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Search for relevant documents\n",
    "    results = search_knowledge_base(question, top_k=2)  # Get top 2 results\n",
    "    \n",
    "    # Display results\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\nüìÑ Result #{i}\")\n",
    "        print(f\"   Score: {result['relevance_score']:.4f}\")\n",
    "        print(f\"   Source: {result['source']} ({result['date']})\")\n",
    "        print(f\"   Content: {result['text']}\")\n",
    "        print()\n",
    "\n",
    "print(\"\\n‚ú® Notice how different questions retrieve different relevant documents!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f63bb6",
   "metadata": {},
   "source": [
    "## Step 6: Understanding RAG Architecture\n",
    "\n",
    "Now let's understand the complete RAG pipeline. So far we've only done **Retrieval**.\n",
    "\n",
    "### The Complete RAG Process:\n",
    "\n",
    "```\n",
    "User Question\n",
    "     ‚Üì\n",
    "1. RETRIEVAL: Search knowledge base for relevant docs\n",
    "     ‚Üì\n",
    "2. AUGMENTATION: Combine question + retrieved docs into a prompt\n",
    "     ‚Üì\n",
    "3. GENERATION: Send to LLM to generate an answer\n",
    "     ‚Üì\n",
    "Final Answer (with sources!)\n",
    "```\n",
    "\n",
    "### Why This is Powerful:\n",
    "- **Without RAG**: LLM only knows what it was trained on (can be outdated or incomplete)\n",
    "- **With RAG**: LLM has access to your specific, up-to-date information\n",
    "- **Bonus**: You can cite sources, making answers verifiable!\n",
    "\n",
    "### Next Steps:\n",
    "To complete RAG, we would:\n",
    "1. Take retrieved documents\n",
    "2. Format them into a prompt like: \"Based on these documents: [docs], answer: [question]\"\n",
    "3. Send to an LLM (like GPT, Claude, or Llama)\n",
    "4. Get back a grounded, cited answer\n",
    "\n",
    "For now, you've learned the hardest part - the retrieval engine!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d22987",
   "metadata": {},
   "source": [
    "## üéì What You've Learned\n",
    "\n",
    "Congratulations! You've built a working retrieval system from scratch. Here's what you now understand:\n",
    "\n",
    "### Programming Concepts:\n",
    "- ‚úÖ **Lists and Dictionaries**: How to structure data\n",
    "- ‚úÖ **Functions**: How to write reusable code with parameters\n",
    "- ‚úÖ **List Comprehensions**: Clean way to transform data `[x for x in items]`\n",
    "- ‚úÖ **Enumerate and Sorting**: How to rank and order results\n",
    "\n",
    "### AI/ML Concepts:\n",
    "- ‚úÖ **Embeddings**: Converting text to meaningful numbers\n",
    "- ‚úÖ **Vector Similarity**: Comparing documents using math (dot product)\n",
    "- ‚úÖ **Normalization**: Making comparisons fair\n",
    "- ‚úÖ **Retrieval**: Finding relevant information efficiently\n",
    "\n",
    "### RAG Pipeline:\n",
    "- ‚úÖ **Knowledge Base**: How to structure document collections\n",
    "- ‚úÖ **Provenance**: Tracking where information comes from\n",
    "- ‚úÖ **Retrieval**: The \"R\" in RAG - finding relevant documents\n",
    "- ‚úÖ **Comparison**: TF-IDF (simple) vs Transformers (advanced)\n",
    "\n",
    "### Next Learning Steps:\n",
    "1. **Add real LLM**: Connect to OpenAI/Anthropic to complete the \"G\" (generation)\n",
    "2. **Scale up**: Use Chroma or Pinecone for larger document collections\n",
    "3. **Add chunking**: Break large documents into smaller pieces\n",
    "4. **Implement reranking**: Use a second model to improve top results\n",
    "5. **Build an agent**: Let the LLM decide when to search vs when it knows the answer\n",
    "\n",
    "## üí° Exercises to Try:\n",
    "\n",
    "1. **Add more documents** to the knowledge base\n",
    "2. **Try your own questions** and see what gets retrieved\n",
    "3. **Experiment with `top_k`** - try returning 1, 3, or 5 results\n",
    "4. **Add a confidence threshold** - only return results above a certain score\n",
    "5. **Track which documents are retrieved most often** (analytics!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0581e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS: Interactive Search Function\n",
    "# Try running this cell and asking your own questions!\n",
    "\n",
    "def interactive_search():\n",
    "    \"\"\"\n",
    "    Let you ask questions interactively and see results.\n",
    "    Type 'quit' to exit.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ü§ñ Interactive Knowledge Base Search\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Ask me anything about the documents in the knowledge base!\")\n",
    "    print(\"Type 'quit' to exit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        # Get user input\n",
    "        question = input(\"Your question: \").strip()\n",
    "        \n",
    "        # Check if user wants to quit\n",
    "        if question.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"üëã Thanks for exploring RAG! Happy learning!\")\n",
    "            break\n",
    "            \n",
    "        # Skip empty questions\n",
    "        if not question:\n",
    "            continue\n",
    "            \n",
    "        # Search the knowledge base\n",
    "        print(\"\\nüîç Searching...\")\n",
    "        results = search_knowledge_base(question, top_k=3)\n",
    "        \n",
    "        # Display results nicely\n",
    "        print(f\"\\nüìö Found {len(results)} relevant documents:\\n\")\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"‚îÅ‚îÅ‚îÅ Result #{i} (Score: {result['relevance_score']:.3f}) ‚îÅ‚îÅ‚îÅ\")\n",
    "            print(f\"üìÑ {result['source']} ({result['date']})\")\n",
    "            print(f\"üí¨ {result['text']}\")\n",
    "            print()\n",
    "\n",
    "# Uncomment the line below to start interactive mode!\n",
    "# interactive_search()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
