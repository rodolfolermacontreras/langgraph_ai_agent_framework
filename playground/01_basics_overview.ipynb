{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f4e6445",
   "metadata": {},
   "source": [
    "# Foundations: Understanding LLMs and Basic Interactions\n",
    "\n",
    "Welcome to your first hands-on notebook for learning AI agents! This notebook will teach you the fundamental building blocks of working with Large Language Models (LLMs) programmatically.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this notebook, you'll understand:\n",
    "1. How to securely load API keys and configuration from environment files\n",
    "2. The message structure used by all modern LLM APIs\n",
    "3. How to design effective prompts that guide model behavior\n",
    "4. How to implement conversation memory to maintain context\n",
    "5. Why these foundations matter for building AI agents\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic Python knowledge (variables, functions, dictionaries, lists)\n",
    "- Understanding that LLMs are AI models that generate text based on input\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Before you can build sophisticated AI agents, you need to understand how to communicate with LLMs. Every agent system - from simple chatbots to complex multi-agent workflows - builds on these core concepts:\n",
    "- Secure configuration management\n",
    "- Message structure and roles\n",
    "- Prompt engineering\n",
    "- Conversation state management\n",
    "\n",
    "Think of this as learning the alphabet before writing sentences. Master these basics, and everything else becomes much easier.\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73313da9",
   "metadata": {},
   "source": [
    "## Step 1: Secure Configuration Management\n",
    "\n",
    "When working with LLMs, you'll need API keys and configuration. NEVER hardcode these in your code - it's a security risk!\n",
    "\n",
    "Instead, we use environment files (.env) that:\n",
    "- Store secrets locally on your machine\n",
    "- Are excluded from version control (via .gitignore)\n",
    "- Can be loaded at runtime\n",
    "\n",
    "Below is a minimal environment loader that doesn't require external dependencies. It searches upward from the current directory to find a .env file, making it robust to where you run the notebook from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c1e25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4 entries from C:\\Training\\Udacity\\AI_Agents_LangGraph\\.env\n"
     ]
    }
   ],
   "source": [
    "# Minimal environment loader that does NOT require python-dotenv\n",
    "# It searches upward from the current working directory for a `.env` file\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def find_upwards(filename='.env', start_dir=None, max_levels=5):\n",
    "    \"\"\"\n",
    "    Search upward through parent directories to find a file.\n",
    "    \n",
    "    Why search upward? When you run a notebook, the current directory might be\n",
    "    the notebook folder, but your .env file is typically at the project root.\n",
    "    This function climbs up the directory tree to find it.\n",
    "    \n",
    "    Args:\n",
    "        filename: Name of file to find (default: '.env')\n",
    "        start_dir: Where to start searching (default: current directory)\n",
    "        max_levels: How many parent directories to check (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "        Path to the file if found, None otherwise\n",
    "    \"\"\"\n",
    "    # Start from specified directory or current working directory\n",
    "    start = Path(start_dir or Path.cwd())\n",
    "    current = start.resolve()  # Get absolute path\n",
    "    \n",
    "    # Search up to max_levels parent directories\n",
    "    for _ in range(max_levels + 1):\n",
    "        candidate = current / filename  # Build full path\n",
    "        if candidate.exists():\n",
    "            return candidate  # Found it!\n",
    "        \n",
    "        # Move to parent directory\n",
    "        if current.parent == current:  # Reached filesystem root\n",
    "            break\n",
    "        current = current.parent\n",
    "    \n",
    "    return None  # File not found\n",
    "\n",
    "def load_dotenv_if_present(dotenv_path=None, max_levels=5):\n",
    "    \"\"\"\n",
    "    Load environment variables from a .env file.\n",
    "    \n",
    "    This function:\n",
    "    1. Finds the .env file (searching upward if needed)\n",
    "    2. Parses each line for KEY=VALUE pairs\n",
    "    3. Sets them as environment variables (accessible via os.environ)\n",
    "    \n",
    "    Args:\n",
    "        dotenv_path: Specific path to .env (optional)\n",
    "        max_levels: How many directories to search upward (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of loaded variables\n",
    "    \"\"\"\n",
    "    # If specific path provided, use it\n",
    "    if dotenv_path:\n",
    "        p = Path(dotenv_path)\n",
    "        if not p.is_absolute():\n",
    "            p = Path.cwd() / p\n",
    "        if not p.exists():\n",
    "            print('No .env file found at', p.resolve())\n",
    "            return {}\n",
    "    else:\n",
    "        # Search upward from current directory\n",
    "        p = find_upwards('.env', start_dir=Path.cwd(), max_levels=max_levels)\n",
    "        if p is None:\n",
    "            print('No .env file found searching up from', Path.cwd())\n",
    "            return {}\n",
    "    \n",
    "    # Parse the .env file\n",
    "    data = {}\n",
    "    with p.open() as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Skip empty lines and comments\n",
    "            if not line or line.startswith('#'):\n",
    "                continue\n",
    "            \n",
    "            # Parse KEY=VALUE format\n",
    "            if '=' not in line:\n",
    "                continue\n",
    "            \n",
    "            key, value = line.split('=', 1)  # Split on first '=' only\n",
    "            key = key.strip()\n",
    "            value = value.strip().strip('\"').strip(\"'\")  # Remove quotes\n",
    "            \n",
    "            # Set as environment variable (only if not already set)\n",
    "            os.environ.setdefault(key, value)\n",
    "            data[key] = value\n",
    "    \n",
    "    print(f'Loaded {len(data)} entries from {p}')\n",
    "    return data\n",
    "\n",
    "# Run loader (safe) - will search up to 5 parent directories by default\n",
    "# This line executes the function and stores results\n",
    "_loaded_env = load_dotenv_if_present()\n",
    "\n",
    "# If no .env file found, that's okay! You can still run this notebook\n",
    "# without connecting to external LLM APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a5282e",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "The code above did three important things:\n",
    "\n",
    "1. **Defined helper functions** to find and load .env files\n",
    "2. **Searched upward** through parent directories to find your .env file\n",
    "3. **Loaded environment variables** into `os.environ` so they're accessible throughout your code\n",
    "\n",
    "**Key Programming Concepts:**\n",
    "- **Path manipulation:** Using `pathlib.Path` for cross-platform file paths\n",
    "- **File I/O:** Reading files line by line\n",
    "- **String parsing:** Splitting KEY=VALUE pairs\n",
    "- **Environment variables:** Setting values in `os.environ`\n",
    "\n",
    "**Security Note:** The .env file is in your `.gitignore`, so it never gets committed to version control. This is how we keep API keys safe!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3f88f7",
   "metadata": {},
   "source": [
    "## Step 2: Understanding LLM Message Structure\n",
    "\n",
    "All modern LLM APIs (OpenAI, Azure, Google, Anthropic) use a similar message structure. Understanding this structure is crucial because it's the foundation of every agent interaction.\n",
    "\n",
    "### Message Roles\n",
    "\n",
    "There are three key roles:\n",
    "\n",
    "1. **system**: Sets the AI's behavior, personality, and constraints\n",
    "   - \"You are a helpful coding assistant\"\n",
    "   - \"You are an expert in Python data analysis\"\n",
    "   - This message is processed first and influences all responses\n",
    "\n",
    "2. **user**: Input from the human or application\n",
    "   - Questions, commands, data to analyze\n",
    "   - What you want the LLM to process\n",
    "\n",
    "3. **assistant**: Outputs from the LLM\n",
    "   - The model's responses\n",
    "   - Used when showing conversation history\n",
    "\n",
    "### Why This Structure Matters\n",
    "\n",
    "The LLM needs context to generate good responses. By maintaining a list of messages with roles, we can:\n",
    "- Show the LLM the conversation history\n",
    "- Control the AI's behavior with system messages\n",
    "- Build multi-turn conversations where the LLM remembers context\n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe0170e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages structure: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Summarize the dataset columns for me.'}]\n"
     ]
    }
   ],
   "source": [
    "# Example: Building a message list for an LLM conversation\n",
    "# Each message is a dictionary with 'role' and 'content' keys\n",
    "\n",
    "messages = [\n",
    "    # System message sets behavior - this is like giving instructions to the AI\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant who explains technical concepts clearly using analogies.\"},\n",
    "    \n",
    "    # User message provides the query or command\n",
    "    {\"role\": \"user\", \"content\": \"Explain what an API is in simple terms.\"},\n",
    "    \n",
    "    # In a real conversation, the assistant's response would be added here\n",
    "    # For example: {\"role\": \"assistant\", \"content\": \"An API is like a waiter in a restaurant...\"}\n",
    "]\n",
    "\n",
    "# Let's examine the structure\n",
    "print(\"Message structure example:\")\n",
    "print(\"-\" * 50)\n",
    "for i, msg in enumerate(messages, 1):\n",
    "    print(f\"\\nMessage {i}:\")\n",
    "    print(f\"  Role: {msg['role']}\")\n",
    "    print(f\"  Content: {msg['content'][:60]}...\" if len(msg['content']) > 60 else f\"  Content: {msg['content']}\")\n",
    "\n",
    "# This is the data structure you'll pass to LLM APIs\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Full structure:\", messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154f5f78",
   "metadata": {},
   "source": [
    "### What You Just Learned\n",
    "\n",
    "You now understand:\n",
    "- **Messages are dictionaries** with \"role\" and \"content\" keys\n",
    "- **System messages control behavior** - they're like giving instructions to an employee\n",
    "- **User messages are inputs** - questions, commands, data\n",
    "- **Assistant messages are outputs** - the LLM's responses\n",
    "- **Order matters** - messages are processed sequentially\n",
    "\n",
    "**In a real application:** You'd send this message list to an LLM API, get back an assistant message, add it to the list, and continue the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d8b137",
   "metadata": {},
   "source": [
    "## Step 3: Prompt Engineering Basics\n",
    "\n",
    "Prompt engineering is the art and science of crafting effective instructions for LLMs. Good prompts get better results.\n",
    "\n",
    "### Key Principles\n",
    "\n",
    "1. **Be specific**: Vague prompts get vague answers\n",
    "2. **Provide context**: The more context, the better the response\n",
    "3. **Use examples**: Show the LLM what you want (few-shot learning)\n",
    "4. **Structure clearly**: Use formatting, sections, constraints\n",
    "\n",
    "### Template Pattern\n",
    "\n",
    "Often you'll want to reuse prompt structures with different values. Python's built-in `string.Template` is perfect for this - no external dependencies needed!\n",
    "\n",
    "Let's create reusable prompt templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de214edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You summarize tables\n",
      "User: Describe columns\n"
     ]
    }
   ],
   "source": [
    "# Prompt template example using built-in Template\n",
    "from string import Template\n",
    "\n",
    "# Create a template with placeholders marked by $variable_name\n",
    "# This allows you to fill in values without rewriting the whole prompt\n",
    "template = Template('''System: $system\n",
    "\n",
    "User: $user_prompt\n",
    "\n",
    "Expected format: $output_format''')\n",
    "\n",
    "# Now we can use this template with different values\n",
    "example_1 = template.substitute(\n",
    "    system='You are a data analyst who summarizes tables clearly',\n",
    "    user_prompt='Describe the columns in this sales dataset',\n",
    "    output_format='Bullet list with column name and data type'\n",
    ")\n",
    "\n",
    "print(\"Example 1 - Data Analysis Prompt:\")\n",
    "print(example_1)\n",
    "print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n",
    "# Same template, different use case\n",
    "example_2 = template.substitute(\n",
    "    system='You are a code reviewer who provides constructive feedback',\n",
    "    user_prompt='Review this Python function for security issues',\n",
    "    output_format='List of issues with severity ratings'\n",
    ")\n",
    "\n",
    "print(\"Example 2 - Code Review Prompt:\")\n",
    "print(example_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee89030",
   "metadata": {},
   "source": [
    "### Understanding Templates\n",
    "\n",
    "**What happened here?**\n",
    "- We defined a template with `$variable_name` placeholders\n",
    "- Used `.substitute()` to fill in the placeholders\n",
    "- Got different prompts from the same template structure\n",
    "\n",
    "**Why this matters:**\n",
    "- **Consistency**: All prompts follow the same structure\n",
    "- **Reusability**: Write the template once, use it many times\n",
    "- **Maintainability**: Update the template in one place\n",
    "- **Testing**: Easy to test different values\n",
    "\n",
    "**Programming Concepts:**\n",
    "- String interpolation with Template\n",
    "- Parameterized text generation\n",
    "- Separation of structure from content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bf0108",
   "metadata": {},
   "source": [
    "## Step 4: Memory Management\n",
    "\n",
    "### Why Memory Matters\n",
    "\n",
    "**The Challenge:**\n",
    "- LLMs are stateless - they don't remember previous conversations\n",
    "- Each call is independent - the model forgets everything after responding\n",
    "- Context window limits - you can't send infinite conversation history\n",
    "\n",
    "**The Solution: Memory Management**\n",
    "- Store conversation history to maintain context\n",
    "- Limit memory to stay within token/character budgets\n",
    "- Convert memory into system prompts for context injection\n",
    "\n",
    "**Real-World Applications:**\n",
    "- Chatbots that remember user preferences\n",
    "- Multi-turn conversations with context\n",
    "- Personalized responses based on history\n",
    "- Debugging and conversation analysis\n",
    "\n",
    "### What You'll Learn\n",
    "- How to build a Memory class from scratch\n",
    "- Character-based memory limiting\n",
    "- Converting history into system prompts\n",
    "- Managing conversation context efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca47644f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System prompt built from memory:\n",
      "Memory summary: I like data visualization and prefer seaborn for quick plots. | Noted — I will suggest charts and colors. | I often work with time series.\n"
     ]
    }
   ],
   "source": [
    "# Memory class that limits stored characters and can produce a system prompt\n",
    "\n",
    "class Memory:\n",
    "    \"\"\"\n",
    "    A simple conversation memory manager with character-based limiting.\n",
    "    \n",
    "    Why we need this:\n",
    "    - LLMs have token limits (typically 4k-128k tokens)\n",
    "    - Can't send infinite conversation history\n",
    "    - Need to keep most relevant/recent information\n",
    "    \n",
    "    How it works:\n",
    "    - Stores messages in chronological order\n",
    "    - Tracks total characters used\n",
    "    - Removes oldest messages when limit exceeded\n",
    "    - Can generate summary for system prompts\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, char_limit=500):\n",
    "        \"\"\"\n",
    "        Initialize memory with a character limit.\n",
    "        \n",
    "        Args:\n",
    "            char_limit: Maximum characters to store (default 500)\n",
    "                       In production, you might use token count instead\n",
    "        \n",
    "        Why char_limit:\n",
    "        - Simple approximation of token usage\n",
    "        - 1 token ≈ 4 characters for English text\n",
    "        - Easy to understand and debug\n",
    "        \"\"\"\n",
    "        self.char_limit = int(char_limit)\n",
    "        self.history = []  # List of message dictionaries\n",
    "        self._total_chars = 0  # Running count of characters\n",
    "    \n",
    "    def add(self, role, content):\n",
    "        \"\"\"\n",
    "        Add a message to memory, removing old messages if limit exceeded.\n",
    "        \n",
    "        Args:\n",
    "            role: 'user', 'assistant', or 'system'\n",
    "            content: The message text\n",
    "        \n",
    "        Process:\n",
    "        1. Create message dictionary\n",
    "        2. Append to history\n",
    "        3. Update character count\n",
    "        4. Remove oldest messages until under limit\n",
    "        \"\"\"\n",
    "        # Create message in same format as LLM APIs\n",
    "        item = {\"role\": role, \"content\": content}\n",
    "        self.history.append(item)\n",
    "        self._total_chars += len(content)\n",
    "        \n",
    "        # Trim oldest messages until we're under the limit\n",
    "        # This is a sliding window approach - FIFO (First In, First Out)\n",
    "        while self._total_chars > self.char_limit and self.history:\n",
    "            removed = self.history.pop(0)  # Remove from beginning\n",
    "            self._total_chars -= len(removed['content'])\n",
    "    \n",
    "    def summarize(self, max_items=5):\n",
    "        \"\"\"\n",
    "        Create a compact summary of recent messages.\n",
    "        \n",
    "        Args:\n",
    "            max_items: Number of recent messages to include\n",
    "        \n",
    "        Returns:\n",
    "            String with messages separated by ' | '\n",
    "        \n",
    "        Why summarize:\n",
    "        - Condense conversation for system prompts\n",
    "        - Keep only most relevant context\n",
    "        - Reduce token usage\n",
    "        \"\"\"\n",
    "        # Take last max_items messages and join their content\n",
    "        return ' | '.join(m['content'] for m in self.history[-max_items:])\n",
    "    \n",
    "    def to_system_prompt(self):\n",
    "        \"\"\"\n",
    "        Convert memory into a system prompt string.\n",
    "        \n",
    "        Returns:\n",
    "            String formatted for LLM system prompt\n",
    "        \n",
    "        Use case:\n",
    "        - Inject conversation context into new LLM calls\n",
    "        - Maintain continuity across sessions\n",
    "        - Provide personalization context\n",
    "        \"\"\"\n",
    "        if not self.history:\n",
    "            return ''\n",
    "        \n",
    "        # Prefix with 'Memory summary:' so LLM knows this is historical context\n",
    "        return 'Memory summary: ' + self.summarize()\n",
    "\n",
    "# Demo: Building conversational memory\n",
    "print(\"=== Memory Management Demo ===\\n\")\n",
    "\n",
    "# Create memory with 200 character limit (small for demo purposes)\n",
    "mem = Memory(char_limit=200)\n",
    "print(f\"Created Memory with {mem.char_limit} character limit\\n\")\n",
    "\n",
    "# Add user preferences (might come from profile or conversation)\n",
    "mem.add('user', 'I like data visualization and prefer seaborn for quick plots.')\n",
    "print(\"Added user preference about seaborn\")\n",
    "\n",
    "# Add assistant acknowledgment\n",
    "mem.add('assistant', 'Noted — I will suggest charts and colors.')\n",
    "print(\"Added assistant response\")\n",
    "\n",
    "# Add more context\n",
    "mem.add('user', 'I often work with time series.')\n",
    "print(\"Added additional user context\\n\")\n",
    "\n",
    "# Check what's in memory\n",
    "print(f\"Current history length: {len(mem.history)} messages\")\n",
    "print(f\"Current character count: {mem._total_chars} / {mem.char_limit}\\n\")\n",
    "\n",
    "# Generate system prompt for next LLM call\n",
    "print('System prompt built from memory:')\n",
    "print(mem.to_system_prompt())\n",
    "print(\"\\nThis would be sent to the LLM to maintain context!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67b1248",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "**Memory Management Flow:**\n",
    "1. Created Memory instance with 200 character limit\n",
    "2. Added 3 messages (user → assistant → user)\n",
    "3. Memory automatically tracked character count\n",
    "4. Generated system prompt from stored history\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Sliding Window**: Old messages dropped when limit exceeded (FIFO)\n",
    "- **Character Counting**: Simple approximation of token usage\n",
    "- **Context Injection**: Memory converted to system prompt\n",
    "- **State Management**: History preserved across interactions\n",
    "\n",
    "**Why This Design:**\n",
    "- Simple and predictable behavior\n",
    "- Easy to understand and debug\n",
    "- Balances context retention with size limits\n",
    "- Ready for production use (with token counting upgrade)\n",
    "\n",
    "**Production Considerations:**\n",
    "- Use token counting instead of characters (tiktoken library)\n",
    "- Add timestamp metadata for time-based filtering\n",
    "- Implement importance scoring (keep critical messages)\n",
    "- Add compression/summarization for very long conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de59c6ea",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "### Exercise 1: Enhanced Memory with Metadata\n",
    "**Objective**: Extend the Memory class to support message tagging and filtering.\n",
    "\n",
    "**Requirements**:\n",
    "- Add a `tags` parameter to the `add()` method\n",
    "- Store tags with each message (e.g., `project`, `priority`, `topic`)\n",
    "- Create a `filter_by_tag()` method that returns messages with specific tags\n",
    "- Update `to_system_prompt()` to accept optional tag filter\n",
    "\n",
    "**Example Usage**:\n",
    "```python\n",
    "mem.add('user', 'Deploy to production', tags=['urgent', 'deployment'])\n",
    "mem.add('user', 'Update documentation', tags=['docs', 'low-priority'])\n",
    "urgent_prompt = mem.to_system_prompt(filter_tags=['urgent'])\n",
    "```\n",
    "\n",
    "**Why This Matters**:\n",
    "- Prioritize important information in prompts\n",
    "- Organize memory by topic/project\n",
    "- Reduce noise in context injection\n",
    "- Support multi-project conversations\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 2: Structured System Prompt Generator\n",
    "**Objective**: Convert memory into formatted, readable system prompts.\n",
    "\n",
    "**Requirements**:\n",
    "- Write a `to_structured_prompt()` method\n",
    "- Format as bullet points or numbered list\n",
    "- Group by role (user preferences, assistant notes, system info)\n",
    "- Include message count and freshness indicators\n",
    "\n",
    "**Example Output**:\n",
    "```\n",
    "User Preferences (2 recent):\n",
    "• Prefers seaborn for visualization\n",
    "• Works with time series data\n",
    "\n",
    "Assistant Notes (1 recent):\n",
    "• Will suggest charts and colors\n",
    "```\n",
    "\n",
    "**Why This Matters**:\n",
    "- Improves LLM comprehension with structure\n",
    "- Makes prompts more readable for debugging\n",
    "- Organizes information by type\n",
    "- Better token efficiency with clear formatting\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 3: Token-Based Memory (Advanced)\n",
    "**Objective**: Replace character counting with actual token counting.\n",
    "\n",
    "**Requirements**:\n",
    "- Install tiktoken: `pip install tiktoken`\n",
    "- Replace `char_limit` with `token_limit`\n",
    "- Use `tiktoken.encoding_for_model()` to get encoder\n",
    "- Count tokens instead of characters in `add()` method\n",
    "\n",
    "**Example Code Start**:\n",
    "```python\n",
    "import tiktoken\n",
    "\n",
    "class TokenMemory(Memory):\n",
    "    def __init__(self, token_limit=500, model=\"gpt-3.5-turbo\"):\n",
    "        self.encoder = tiktoken.encoding_for_model(model)\n",
    "        # Continue implementation...\n",
    "```\n",
    "\n",
    "**Why This Matters**:\n",
    "- Accurate token usage tracking\n",
    "- Works with actual LLM limits\n",
    "- Model-specific tokenization\n",
    "- Production-ready implementation\n",
    "\n",
    "---\n",
    "\n",
    "### Challenge Exercise: Intelligent Memory Compression\n",
    "**Objective**: Add automatic summarization when memory limit approached.\n",
    "\n",
    "**Requirements**:\n",
    "- Detect when approaching limit (e.g., 80% full)\n",
    "- Use an LLM to summarize older messages\n",
    "- Replace old messages with compressed summary\n",
    "- Preserve most recent messages unchanged\n",
    "\n",
    "**Why This Matters**:\n",
    "- Maintain longer conversation context\n",
    "- Automatic context management\n",
    "- Balance detail with efficiency\n",
    "- Advanced production pattern\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue with [02_code_examples.ipynb](02_code_examples.ipynb) for:\n",
    "- Runnable code examples with tool calling\n",
    "- Minimal agent loop implementation\n",
    "- Hands-on practice with function stubs\n",
    "- Building your first working agent\n",
    "\n",
    "**What You've Learned So Far**:\n",
    "1. Secure configuration management with .env files\n",
    "2. LLM message structure and roles\n",
    "3. Prompt engineering with templates\n",
    "4. Memory management and context injection\n",
    "\n",
    "You now have the foundational knowledge to build stateful AI agents!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
