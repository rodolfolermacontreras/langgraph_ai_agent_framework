{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f4e6445",
   "metadata": {},
   "source": [
    "# 01 — Basics Overview (Runnable)\n",
    "\n",
    "This notebook contains runnable examples that work without external LLM SDKs. It includes a tiny environment loader (no dependencies required), a message structure demo, prompt templating, and an improved `Memory` class that limits by characters. If you want to connect to a real LLM (Foundry or similar), see the `docs/roadmap.md` for secure configuration using a local `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87c1e25b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T01:07:29.644138Z",
     "iopub.status.busy": "2025-12-18T01:07:29.644138Z",
     "iopub.status.idle": "2025-12-18T01:07:29.666844Z",
     "shell.execute_reply": "2025-12-18T01:07:29.666844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4 entries from C:\\Training\\Udacity\\AI_Agents_LangGraph\\.env\n"
     ]
    }
   ],
   "source": [
    "# Minimal environment loader that does NOT require python-dotenv\n",
    "# It searches upward from the current working directory for a `.env` file\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def find_upwards(filename='.env', start_dir=None, max_levels=5):\n",
    "    start = Path(start_dir or Path.cwd())\n",
    "    current = start.resolve()\n",
    "    for _ in range(max_levels + 1):\n",
    "        candidate = current / filename\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "        if current.parent == current:\n",
    "            break\n",
    "        current = current.parent\n",
    "    return None\n",
    "\n",
    "def load_dotenv_if_present(dotenv_path=None, max_levels=5):\n",
    "    # If dotenv_path is provided and exists, use it. Otherwise search upwards from cwd.\n",
    "    if dotenv_path:\n",
    "        p = Path(dotenv_path)\n",
    "        if not p.is_absolute():\n",
    "            p = Path.cwd() / p\n",
    "        if not p.exists():\n",
    "            print('No .env file found at', p.resolve())\n",
    "            return {}\n",
    "    else:\n",
    "        p = find_upwards('.env', start_dir=Path.cwd(), max_levels=max_levels)\n",
    "        if p is None:\n",
    "            print('No .env file found searching up from', Path.cwd())\n",
    "            return {}\n",
    "    data = {}\n",
    "    with p.open() as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('#'):\n",
    "                continue\n",
    "            if '=' not in line:\n",
    "                continue\n",
    "            k, v = line.split('=', 1)\n",
    "            k = k.strip()\n",
    "            v = v.strip().strip('\"').strip('\"')\n",
    "            os.environ.setdefault(k, v)\n",
    "            data[k] = v\n",
    "    print('Loaded', len(data), 'entries from', p)\n",
    "    return data\n",
    "\n",
    "# Run loader (safe) — will search up to 5 parent directories by default\n",
    "_loaded_env = load_dotenv_if_present()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3f88f7",
   "metadata": {},
   "source": [
    "## LLM Chat Structure (Conceptual)\n",
    "\n",
    "LLM chat systems use a list of messages with roles: `system`, `user`, and `assistant`. The `system` message sets behavior, `user` provides instructions or queries, and `assistant` contains model outputs. We'll demonstrate how this structure maps to function calls and simple agent loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afe0170e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T01:07:29.670078Z",
     "iopub.status.busy": "2025-12-18T01:07:29.670078Z",
     "iopub.status.idle": "2025-12-18T01:07:29.678327Z",
     "shell.execute_reply": "2025-12-18T01:07:29.678327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages structure: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Summarize the dataset columns for me.'}]\n"
     ]
    }
   ],
   "source": [
    "# Example: message structure example (no external API)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Summarize the dataset columns for me.\"},\n",
    "]\n",
    "print('Messages structure:', messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d8b137",
   "metadata": {},
   "source": [
    "## Prompt Design (Short)\n",
    "Prompts guide model behavior. Start with a clear `system` instruction, then provide context and ask concise questions. Use few-shot examples for structure when needed. The example below uses Python's `string.Template` which is built-in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de214edb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T01:07:29.681998Z",
     "iopub.status.busy": "2025-12-18T01:07:29.678327Z",
     "iopub.status.idle": "2025-12-18T01:07:29.685788Z",
     "shell.execute_reply": "2025-12-18T01:07:29.685788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You summarize tables\n",
      "User: Describe columns\n"
     ]
    }
   ],
   "source": [
    "# Prompt template example using built-in Template\n",
    "from string import Template\n",
    "template = Template('System: $system\\nUser: $user_prompt')\n",
    "print(template.substitute(system='You summarize tables', user_prompt='Describe columns'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bf0108",
   "metadata": {},
   "source": [
    "## Memory (Runnable Implementation)\n",
    "Memory stores the conversation history or facts about the user. Below is a small implementation that limits stored characters (not messages) and can convert memory into a compact system prompt for future LLM calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca47644f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T01:07:29.690540Z",
     "iopub.status.busy": "2025-12-18T01:07:29.689450Z",
     "iopub.status.idle": "2025-12-18T01:07:29.697484Z",
     "shell.execute_reply": "2025-12-18T01:07:29.697484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System prompt built from memory:\n",
      "Memory summary: I like data visualization and prefer seaborn for quick plots. | Noted — I will suggest charts and colors. | I often work with time series.\n"
     ]
    }
   ],
   "source": [
    "# Memory class that limits stored characters and can produce a system prompt\n",
    "class Memory:\n",
    "    def __init__(self, char_limit=500):\n",
    "        self.char_limit = int(char_limit)\n",
    "        self.history = []\n",
    "        self._total_chars = 0\n",
    "    def add(self, role, content):\n",
    "        item = {\"role\": role, \"content\": content}\n",
    "        self.history.append(item)\n",
    "        self._total_chars += len(content)\n",
    "        # Trim oldest until under limit\n",
    "        while self._total_chars > self.char_limit and self.history:\n",
    "            removed = self.history.pop(0)\n",
    "            self._total_chars -= len(removed['content'])\n",
    "    def summarize(self, max_items=5):\n",
    "        return ' | '.join(m['content'] for m in self.history[-max_items:])\n",
    "    def to_system_prompt(self):\n",
    "        # Convert recent memory into a single system prompt string\n",
    "        if not self.history:\n",
    "            return ''\n",
    "        return 'Memory summary: ' + self.summarize()\n",
    "\n",
    "# Demo\n",
    "mem = Memory(char_limit=200)\n",
    "mem.add('user', 'I like data visualization and prefer seaborn for quick plots.')\n",
    "mem.add('assistant', 'Noted — I will suggest charts and colors.')\n",
    "mem.add('user', 'I often work with time series.')\n",
    "print('System prompt built from memory:')\n",
    "print(mem.to_system_prompt())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de59c6ea",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "- Modify the `Memory` class to add metadata tags (e.g., `project`, `priority`) to messages and filter by tag when building a system prompt.\n",
    "- Write a function that converts the memory into a structured system prompt with bullet points for the LLM.\n",
    "\n",
    "---\n",
    "End of `01_basics_overview.ipynb`. Continue with `02_code_examples.ipynb` for runnable code examples calling local dummy tools and structured explanations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
